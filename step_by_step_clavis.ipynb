{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Working Clavis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nProduct - URL\\tModel - URL\\tStyle - URL\\tGender - URL\\tSport - URL\\tBest for - URL\\tColour - URL\\tFeatures - URL\\tCollection - URL\\tBrand - URL\\tSize - URL\\tRise - URL\\tSustainable - URL\\tMaterial - URL\\tTeams - URL\\tKit Teams - URL\\tWinter - URL\\tOutlet - URL\\tSupport - URL\\tLength - URL\\tFit - URL\\tSurface - URL\\tTechologies - URL\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This is Clavis\n",
    "## Clavis is a Keyword Expansion and Categorization Tool for Digital Marketing & URL Generation\n",
    "\n",
    "## For example if you have a keyword like \"best shoes\" and you want to expand it to \"best shoes for cycling\" and \"best shoes for running\"\n",
    "## as well as categorize whether it is \"Male\", \"Female\", \"Unisex\" or \"Kids\" you can do that with Clavis\n",
    "## Also identify whether a keyword is Generic or Branded (Generic = \"best shoes\", Branded = \"Nike shoes\")\n",
    "## also it identifies the Topic of the keyword (\"Accessories\", \"Cycling Jackets\", \"Mountain Biking Jackets\", etc..)\n",
    "## lastly it will also identify which ones should be \"Deleted\" and which ones should be \"Kept\"\n",
    "\n",
    "## The input to this system are two different CSV Files:\n",
    "\n",
    "## Input 1: Keywords.csv\n",
    "## Keywords File: This is a CSV file with the following columns: Keywords\n",
    "## Keywords File Example:\n",
    "## Keywords: Best shoes, running shoes, etc..\n",
    "\n",
    "## Input 2: Categories.csv\n",
    "## Categories File: This is a CSV with the following columns: Category, Search For, Return\n",
    "## Categories File Example:\n",
    "## Category | Search For | Return\n",
    "## Gender | | Unisex --> If Search For is empty, return Unisex\n",
    "## Gender | man | Men\n",
    "## Gender | woman | Women\n",
    "## Gender | lady | Women\n",
    "## Gender | boy | Kids\n",
    "## Branded_generic | | Generic --> If Search For is empty, return Generic\n",
    "## Branded_generic | nike | Branded\n",
    "## Branded_generic | adidas | Branded\n",
    "## Branded_generic | puma | Branded\n",
    "## Branded_generic | reebok | Branded\n",
    "## Topic | | None --> If Search For is empty, return None\n",
    "## Topic | cleats | accessories\n",
    "## topic | vest | Gilet\n",
    "## Topic | jacket*cycling*mountain*jacket | mountain biking jackets --> asterisk is a wildcard character, it can be used to match multiple words\n",
    "\n",
    "## Output:\n",
    "## Output is a CSV file with the following columns: Keywords, Gender, Branded_Generic, Topic, Delete_Keep\n",
    "## Output Example:\n",
    "## Keywords | Gender | Branded_Generic | Topic | Delete_Keep\n",
    "## Best shoes | Unisex | Generic | None | Keep\n",
    "## Best shoes for cycling | Unisex | Generic | cycling | Keep\n",
    "## Best shoes for running | Unisex | Generic | running | Keep\n",
    "## Nike shoes | Unisex | Branded | None | Keep\n",
    "## Nike shoes for cycling | Unisex | Branded | cycling | Keep\n",
    "## Nike shoes for running | Unisex | Branded | running | Keep\n",
    "## Adidas shoes | Unisex | Branded | None | Keep\n",
    "## women's cycling suit | Women | Generic | cycling | Keep\n",
    "\n",
    "\n",
    "## PART 3: URL Generation\n",
    "\"\"\"\n",
    "\n",
    "Product - URL\tModel - URL\tStyle - URL\tGender - URL\tSport - URL\tBest for - URL\tColour - URL\tFeatures - URL\tCollection - URL\tBrand - URL\tSize - URL\tRise - URL\tSustainable - URL\tMaterial - URL\tTeams - URL\tKit Teams - URL\tWinter - URL\tOutlet - URL\tSupport - URL\tLength - URL\tFit - URL\tSurface - URL\tTechologies - URL\n",
    "\"\"\"\n",
    "## those are all the mappings that are available for the URL Generation\n",
    "## the matching will be done for each of the categories and then concatenated with a custom separator (default is \"-\")\n",
    "\n",
    "## There are two parts to this:\n",
    "## Part 1. Keyword Search Volume & Expansion This is done with the custom scripts we have under KeywordSearchVolume module [DONE]\n",
    "## Part 2. Keyword Categorization This is done in the aforementioned fashion from the two CSV Files [DONE]\n",
    "## Part 3. URL Generation - This is done with an extra mapping file that maps the keywords to the URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing the libraries\n",
    "import os\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## custom modules\n",
    "from KeywordSearchVolume.search_volume_extractor import run_search_volume\n",
    "from clavis_helpers import (\n",
    "    does_file_exist,\n",
    "    parse_search_volume,\n",
    "    load_excel_and_clean,\n",
    "    get_payload,\n",
    "    clean_categories_df,\n",
    "    categorize_keywords,\n",
    "    apply_clean_dataframe_for_categorization,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PATHS & PARAMETERS\n",
    "\n",
    "SAVE_DIR = \"./sample_data\"\n",
    "\n",
    "## Part 1. Keyword Search Volume & Expansion\n",
    "LANGUAGE = \"English\"  ## this has to be a streamlit input\n",
    "EXPAND_KEYWORDS = True  ## this has to be a streamlit radio button\n",
    "GEOLOCATION = \"United States\"  ## this has to be a streamlit input\n",
    "\n",
    "## CLAVIS CONFI FILES\n",
    "CLAVIS_CONFIG_NAME = \"Keyword-Categorization-Mapping-Config.xlsx\"\n",
    "CLAVIS_CONFIG_SHEET_NAME = \"Config - Categorisation\"\n",
    "CLAVIS_CONFIG_FILE_PATH = os.path.join(SAVE_DIR, CLAVIS_CONFIG_NAME)\n",
    "\n",
    "## KEYWORDS AND CATEGORY FILES\n",
    "KEYWORDS_FILE_NAME = \"Keywords.csv\"\n",
    "KEYWORDS_FILE_PATH = os.path.join(SAVE_DIR, KEYWORDS_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part 0. Load the files\n",
    "catz = pd.read_csv(f\"{SAVE_DIR}/categories.csv\", sep=\";\")\n",
    "KEYWORDS_FILE = pd.read_csv(KEYWORDS_FILE_PATH)\n",
    "\n",
    "## get the payload\n",
    "PAYLOAD = get_payload(KEYWORDS_FILE, LANGUAGE, EXPAND_KEYWORDS, GEOLOCATION)\n",
    "\n",
    "\n",
    "## parse the search volume\n",
    "if not does_file_exist(f\"{SAVE_DIR}/intermediate_results.csv\"):\n",
    "    ## run the search volume\n",
    "    search_volume = run_search_volume(**PAYLOAD)\n",
    "    search_volume_df = parse_search_volume(search_volume)\n",
    "else:\n",
    "    search_volume_df = pd.read_csv(f\"{SAVE_DIR}/intermediate_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the clavis config file\n",
    "clavis_config = load_excel_and_clean(CLAVIS_CONFIG_FILE_PATH, CLAVIS_CONFIG_SHEET_NAME)\n",
    "\n",
    "## function to perform the above steps\n",
    "separated_categories_dict = clean_categories_df(clavis_config)\n",
    "\n",
    "## cleaned dataframe\n",
    "cleaned_categories_df = apply_clean_dataframe_for_categorization(\n",
    "    separated_categories_dict\n",
    ")\n",
    "\n",
    "## separate the URL generation from the categorization\n",
    "url_generation_df = cleaned_categories_df[\n",
    "    cleaned_categories_df[\"Category\"].str.contains(\"URL\")\n",
    "]\n",
    "\n",
    "## separate the categorization from the URL generation\n",
    "categorization_df = cleaned_categories_df[\n",
    "    ~cleaned_categories_df[\"Category\"].str.contains(\"URL\")\n",
    "]\n",
    "\n",
    "categorized_keywords = categorize_keywords(search_volume_df, categorization_df)\n",
    "\n",
    "## now URL generation\n",
    "generated_urls = categorize_keywords(search_volume_df, url_generation_df)\n",
    "\n",
    "## get the columns that are not keywords, search volume, idea, or 0\n",
    "columns_to_concatenate = generated_urls.columns[\n",
    "    ~generated_urls.columns.isin([\"keywords\", \"search_volume\", \"idea\"])\n",
    "]\n",
    "\n",
    "## in all of the columns to concatenate, replace 0 with \"\", 0 is a str (\"0\")\n",
    "generated_urls[columns_to_concatenate] = generated_urls[columns_to_concatenate].replace(\n",
    "    \"0\", \"\"\n",
    ")\n",
    "\n",
    "# ## concatenate the columns\n",
    "generated_urls[\"url\"] = generated_urls[columns_to_concatenate].apply(\n",
    "    lambda x: \"-\".join(x.dropna().astype(str)), axis=1\n",
    ")\n",
    "\n",
    "## cleaning - remove the hyphen\n",
    "generated_urls[\"url\"] = generated_urls[\"url\"].replace(\n",
    "    \"-\", \"\", regex=True, inplace=False\n",
    ")\n",
    "\n",
    "## replace the ampersand & with \"\"\n",
    "generated_urls[\"url\"] = generated_urls[\"url\"].replace(\n",
    "    \"&\", \"\", regex=True, inplace=False\n",
    ")\n",
    "\n",
    "## join back the two dataframes\n",
    "final_df = pd.merge(\n",
    "    categorized_keywords,\n",
    "    generated_urls,\n",
    "    on=[\"keywords\", \"search_volume\", \"idea\"],\n",
    "    how=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = \"/Users/eric/Documents/Locaria/Projects/Clavis/allowed_langs_locs/locations_languages.csv\"\n",
    "df = pd.read_csv(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n5/8tz171ws7vxgbsjsh88bv45h0000gn/T/ipykernel_56831/1360654322.py:2: UserWarning: DataFrame columns are not unique, some columns will be omitted.\n",
      "  lang_loc_dict = df.set_index(\"location_name\").T.to_dict('list')\n"
     ]
    }
   ],
   "source": [
    "## convert the dataframe entries to a dictionary\n",
    "lang_loc_dict = df.set_index(\"location_name\").T.to_dict(\"list\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_loc_dict = df.groupby([\"location_name\"])[\"language_name\"].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the dict as a json file\n",
    "import json\n",
    "\n",
    "with open(\"./allowed_langs_locs/lang_loc_dict.json\", \"w\") as fp:\n",
    "    json.dump(lang_loc_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import open_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open_json(\"./allowed_langs_locs/lang_loc_dict.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Algeria': ['French', 'Arabic'],\n",
       " 'Angola': ['Portuguese'],\n",
       " 'Argentina': ['Spanish'],\n",
       " 'Armenia': ['Armenian'],\n",
       " 'Australia': ['English'],\n",
       " 'Austria': ['German'],\n",
       " 'Azerbaijan': ['Azeri'],\n",
       " 'Bahrain': ['Arabic'],\n",
       " 'Bangladesh': ['Bengali'],\n",
       " 'Belgium': ['French', 'Dutch', 'German'],\n",
       " 'Bolivia': ['Spanish'],\n",
       " 'Brazil': ['Portuguese'],\n",
       " 'Bulgaria': ['Bulgarian'],\n",
       " 'Burkina Faso': ['French'],\n",
       " 'Cambodia': ['English'],\n",
       " 'Cameroon': ['French'],\n",
       " 'Canada': ['English', 'French'],\n",
       " 'Chile': ['Spanish'],\n",
       " 'Colombia': ['Spanish'],\n",
       " 'Costa Rica': ['Spanish'],\n",
       " \"Cote d'Ivoire\": ['French'],\n",
       " 'Croatia': ['Croatian'],\n",
       " 'Cyprus': ['Greek', 'English'],\n",
       " 'Czechia': ['Czech'],\n",
       " 'Denmark': ['Danish'],\n",
       " 'Ecuador': ['Spanish'],\n",
       " 'Egypt': ['Arabic', 'English'],\n",
       " 'El Salvador': ['Spanish'],\n",
       " 'Estonia': ['Estonian'],\n",
       " 'Finland': ['Finnish'],\n",
       " 'France': ['French'],\n",
       " 'Germany': ['German'],\n",
       " 'Ghana': ['English'],\n",
       " 'Greece': ['Greek'],\n",
       " 'Guatemala': ['Spanish'],\n",
       " 'Hong Kong': ['English', 'Chinese (Traditional)'],\n",
       " 'Hungary': ['Hungarian'],\n",
       " 'India': ['English', 'Hindi'],\n",
       " 'Indonesia': ['English', 'Indonesian'],\n",
       " 'Ireland': ['English'],\n",
       " 'Israel': ['Hebrew', 'Arabic'],\n",
       " 'Italy': ['Italian'],\n",
       " 'Japan': ['Japanese'],\n",
       " 'Jordan': ['Arabic'],\n",
       " 'Kazakhstan': ['Russian'],\n",
       " 'Kenya': ['English'],\n",
       " 'Latvia': ['Latvian'],\n",
       " 'Lithuania': ['Lithuanian'],\n",
       " 'Malaysia': ['English', 'Malay'],\n",
       " 'Malta': ['English'],\n",
       " 'Mexico': ['Spanish'],\n",
       " 'Morocco': ['Arabic'],\n",
       " 'Myanmar (Burma)': ['English'],\n",
       " 'Netherlands': ['Dutch'],\n",
       " 'New Zealand': ['English'],\n",
       " 'Nicaragua': ['Spanish'],\n",
       " 'Nigeria': ['English'],\n",
       " 'North Macedonia': ['Macedonian'],\n",
       " 'Norway': ['Norwegian'],\n",
       " 'Pakistan': ['English', 'Urdu'],\n",
       " 'Panama': ['Spanish'],\n",
       " 'Paraguay': ['Spanish'],\n",
       " 'Peru': ['Spanish'],\n",
       " 'Philippines': ['English', 'Tagalog'],\n",
       " 'Poland': ['Polish'],\n",
       " 'Portugal': ['Portuguese'],\n",
       " 'Romania': ['Romanian'],\n",
       " 'Saudi Arabia': ['Arabic'],\n",
       " 'Senegal': ['French'],\n",
       " 'Serbia': ['Serbian'],\n",
       " 'Singapore': ['English', 'Chinese (Simplified)'],\n",
       " 'Slovakia': ['Slovak'],\n",
       " 'Slovenia': ['Slovenian'],\n",
       " 'South Africa': ['English'],\n",
       " 'South Korea': ['Korean'],\n",
       " 'Spain': ['Spanish'],\n",
       " 'Sri Lanka': ['English'],\n",
       " 'Sweden': ['Swedish'],\n",
       " 'Switzerland': ['German', 'French', 'Italian'],\n",
       " 'Taiwan': ['Chinese (Traditional)'],\n",
       " 'Thailand': ['Thai'],\n",
       " 'Tunisia': ['Arabic'],\n",
       " 'Turkey': ['Turkish'],\n",
       " 'Ukraine': ['Ukrainian', 'Russian'],\n",
       " 'United Arab Emirates': ['Arabic', 'English'],\n",
       " 'United Kingdom': ['English'],\n",
       " 'United States': ['English', 'Spanish'],\n",
       " 'Uruguay': ['Spanish'],\n",
       " 'Venezuela': ['Spanish'],\n",
       " 'Vietnam': ['English', 'Vietnamese']}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Spanish'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"United States\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('Clavis-GswOuXK2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8091dbd070472be2363e6046d953fa5841dfc81f86941db68117ba11a41bd54e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
